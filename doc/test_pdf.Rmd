---
title: "Project 4, Who is who"
author: "Ruxue Peng,rp2815"
date: "4/13/2017"
output: html_document
---  
In this project, we implement two of the suggested papers:  
(1) Han, Hui, et al. "Two supervised learning approaches for name disambiguation in author citations."  2004   
(2) Culotta, Aron, et al. "Author disambiguation using error-driven machine learning with a ranking loss function." 2007  

#Error Driven Paper 
The algorithm is "C_E_Pr"  
C: Clustering  
E:Error Driven  
Pr: Ranking perceptron lamda_t+1 = lamda_t + F(T)  

Basically, we defined a scoring system and performed agglomerative clustering on paper citations. We updated the scoring weight $lamda$ through ranking perceptron scheme. At last, we output the best $lamda$ for the scoring system so that we could find the best clustering partition in test set.  

## Step 0: Load the packages and fucntions

```{r, eval=FALSE}
packages.used=c("plyr","dplyr","text2vec","qlcMatrix","kernlab", "parallel")
# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE,
                   repos='http://cran.us.r-project.org')
}

library(plyr)
library(text2vec)
library(qlcMatrix)
library(kernlab)
library(dplyr)
library(parallel)
source("../lib/get_F1.R")
source("../lib/get_F1_accu.R")
source("../lib/get_labels_individual.R")
source("../lib/get_score_individual.R")
source("../lib/create_overlap.R")
source("../lib/Split_individual.R")
source("../lib/get_GoldStand.R")
source("../lib/hier_clus_train_individual.R")
source("../lib/hier_clus_for_test.R")
source("../lib/create_journal.R")
source("../lib/create_paper.R")
source("../lib/get_feature_individual.R")
source("../lib/Hier_clus_for_test.R")


```

## Step 1: Data Processing - Parse Txt Data
Following the experiment section in the paper, we used coauthors' names, paper titles as well as journal titles to design features for citations. A function create_data() is written to automate this.  

```{r,eval=FALSE}
#function for data pre-processing
create_data <- function(filename){
  #name <- deparse(substitute(filename))
  tmp <- read.csv(filename,
                  header = F,
                  sep = "\n")    
  rule = "<([[:alpha:]]|[[:punct:]]){1,4}>"
  tmp$V1 = gsub(rule,"",tmp$V1)
  rule1 = ">([[:alpha:]]){1,5}:"
  tmp$V1 = gsub(rule1,">",tmp$V1)
  Sys.setlocale('LC_ALL','C')
  L = strsplit(tmp$V1,split = ">")
  tmp$Coauthor = laply(L,function(t) t[1])
  tmp$Paper = laply(L,function(t) t[2])
  tmp$Journal = laply(L,function(t) t[3])
  
  # extract canonical author id befor "_"
  tmp$AuthorID <- as.numeric(sub("_.*","",tmp$Coauthor))
  # extract paper number under same author between "_" and first whitespace
  tmp$PaperNO <- as.numeric(sub(".*_(\\w*)\\s.*", "\\1", tmp$Coauthor))
  # delete "<" in AKumar$Coauthor, you may need to further process the coauthor
  # term depending on the method you are using
  tmp$Coauthor <- gsub("<","",sub("^.*?\\s","", tmp$Coauthor))
  # delete "<" in AKumar$Paper
  tmp$Paper <- gsub("<","",tmp$Paper)
  # add PaperID for furthur use, you may want to combine all the nameset files and 
  # then assign the unique ID for all the citations
  tmp$PaperID <- rownames(tmp)
  tmp = tmp[,-1]
  return(tmp)
}
```  
```{r,eval=FALSE}
## apply function
setwd("../data/nameset")
file_names <- list.files(pattern = "*.txt")
#file_names = file_names[-c(6,7,10)]
Data = list()
for(i in 1:length(file_names)){
  Data[[i]]= create_data(file_names[i])
}
names(Data) = file_names
rm(i)
```  

## Step 2: Feature Construction and PCA selection

We compute the number of overlapping coauthors between 2 papers, the "TF-IDF" of paper titles and also Journal titles as suggested in the paper.   
About "TF-IDF" (term frequency-inverse document frequency) 

TF-IDF is a numerical statistics that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in information retrieval, text mining, and user modeling. The TF-IDF value increases proportionally to the number of times a word appears in the document, but is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general.

$$
\begin{aligned}
\mbox{TF}(t) &=\frac{\mbox{Number of times term $t$ appears in a document}}{\mbox{Total number of terms in the document}}\\
\mbox{IDF}(t) &=\log{\frac{\mbox{Total number of documents}}{\mbox{Number of documents with term $t$ in it}}}\\
\mbox{TF-IDF}(t) &=\mbox{TF}(t)\times\mbox{IDF}(t)
\end{aligned}
$$   

### Feature Construction and Reduction   

We use PCA to reduce features and wrote feature construction function: get_feature(). For details of the function, please go to ../lib/get_feature.R 

```{r,eval=FALSE}
# f returns the PCAs with true labels on the 1st column
Fea = llply(Data,get_feature)
```  

### Split Train/Test  

We split each person's papers into half and half, put the first half in training set and the other half in the testing set.  

```{r,eval=FALSE}
##Split train and test
Train = llply(Fea,Split_Train)
Test = llply(Fea,Split_Test)
```  

## Step 3: Clustering  
Here we are trying to construct a scoring system that approximates the true score of a clustering method. If we have a good scoring system, we'll be able to pick the clustering that yields the highest score, and use its parition as prediction on the testing data, whose true assignments are unknown.  

In order to do that, we need to update the scoring weight (named $Lamda$) of each features.  

### Error Driven Training  
If our scoring system picks a clustering partition that is the best, we don't have to update lamda any more. If some other clustering method outperforms it, then it is said that we have an error and thus have to update our scoring weight. This is the basic idea of error driven online training. 
We will adopt hard start on the clustering, meaning that we'll use the true labels to create a base partitioning using kmeans.    
Then we use the base as a start for the hierarchical clustering.  

### Function for updating lamda   
To see the details of functions used within, please go to lib folder.  

```{r}
Hier_clus= function(Fea_wl,Fea_wol,Lamda,data = Train[[1]]){
  #get golden standard
  G = get_GoldStand(data)
  y = G$y
  a_c =G$a_c
  b_d = G$b_d 
  
  #hard start
  lamda = Lamda
  N = length(unique(y)) + 10
  init_cluster = kmeans(Fea_wl,centers = N,nstart =1)
  # initialization
  label_i= init_cluster$cluster
  
  for (v in 1: (N -length(unique(y)))){
    labels= get_labels(init_label= label_i,data = data)
    for(j in 1:5){
    #Parallel processing for greedy search
    cores <- detectCores()-1
    cl1 = makeCluster(cores,type = "FORK")
    score<- parApply(cl1,labels,2,get_score,lamda,Fea_wol)
    stopCluster(cl1)
    #score <- aaply(labels,2,get_score,lamda,Fea_wol)
      # extract the best score and N_hat and its F1 score
      ind_N_hat <- which.max(score)
      #ind_N_star <- which(F1==max(F1))
      label_i <-labels[,ind_N_hat]
      F1_N_hat = get_F1(label_i,A_C = a_c,B_D = b_d, data = data)
      
      # extract N_star
      for(i in 1:ncol(labels)){
        tmp = get_F1(init_label = labels[,i],A_C = a_c,B_D = b_d, data = data)
        if(tmp > F1_N_hat){
          ind_N_star = i
          break()
        }
        else{ind_N_star = ind_N_hat}
      }
      label_j <- labels[,ind_N_star]
      # update lamda
      Fea_wol_star <- data.frame(cbind(label_1=label_i,Fea_wol))
      Fea_wol_hat <- data.frame(cbind(label_1=label_j,Fea_wol))
      F_T_star <- colSums(aggregate(Fea_wol_star,by=list(Fea_wol_star$label_1),FUN=mean))[-c(1,2)]
      F_T_hat <- colSums(aggregate(Fea_wol_hat,by=list(Fea_wol_hat$label_1),FUN=mean))[-c(1,2)]
      incre <- sqrt(sum((F_T_hat-F_T_star)^2))
      lamda = lamda + 0.8*F_T_star - 0.8*F_T_hat
    }
  }
  return(lamda)
}

```  

### Train lamdas for all 14 txts  
Since each txt contains different paper title and journal title, they gave different features. Using the same lamda will not be reasonable and the length will not match. Therefore, we trained 14 different lamdas utilizing 14 txt files.  

```{r,eval=FALSE}
# initialize lamdas
best_lamda = llply(Train,function(t) rep(1/ncol(t[,-1]),ncol(t[,-1])))
Big_list = list()
for (i in 1:14) {
  Lam = matrix(rep(best_lamda6[[i]],nrow(Train[[i]])),byrow = T,nrow = nrow(Train[[i]]))
  Big_mat = cbind(Train[[i]],Lam)
  Big_list[[i]] =  Big_mat 
}
# store the 14 best_lamda vectors here:
best_lamda = llply(Big_list,function(t) Hier_clus(Fea_wl = t[,1:((ncol(t)+1)/2)],Fea_wol = t[,2:((ncol(t)+1)/2)],Lamda = t(t[1,((ncol(t)+3)/2):ncol(t)]),data = t[,1:((ncol(t)+1)/2)]))
```  

### Test on the other half of the data  

```{r,eval=FALSE}
#test set
# function Hier_clus_test() returns the accuracy and pairwise F1 score of the clustering partition chosen using best_lamda
test6 = Hier_clus_test(Test[[6]],Test[[6]][,-1],best_lamda[[6]],data = Test[[6]])
#accuracy = 60.3%
```  

## Step 4: Evaluation using pairwise F1 score  

Here we evaluate the algorithm itself, looking at its performance on different datasets, various feature types and combination of input parameters.  
We chose F1 score over accuracy, because the author informed us that they used pairwise F1 score as s_star and suggested that we should use that for evaluation. To better reproduce the result, we keep the criteria the same as before.  

### Different dataset  

Obviously, this does not do justice to the CEPr algorithm. If computation allows, better F1 can be obtained.
The variance of these F1s replects the algorithm's performance on different datasets, some have authors that wrote very few papers. After observing the txts, it is safe to say that the algorithm prefers dataset that have a lot of citations per person, and equal citation amounts between different persons.  

```{r,echo=FALSE}
result_cepr = read.table("../data/test_result_3predictor_cepr.csv",as.is = T,sep = ",",header = T)
colnames(result_cepr)[1] = "TXT_ID"
result_cepr = na.omit(result_cepr)
result_cepr[,1:2]
```  

### Different input parameters
We designed a way to evaluate the effect of Error Driven Online Training, excluding the effect of kmeans hard start.  
We train using the fifth txt, and two separate input parameters, for the first one, we update lamda twice at each level, for the other, 5 times.  
It is observed that the more we update lamda, the better F1 score we get.  

```{r}
F1_compare = c(Update_2 = 0.205, Update_5 = 0.232)
F1_compare
```  
### Different features  
The algorithm is not that sensitive to feature changes.  

```{r,echo=FALSE,warning=FALSE}
result = read.table("../data/test_result_1predictor.csv",as.is = T,sep = ",",header = T)
colnames(result) = c("TXT_ID", "CEPr_Coauthor"      ,      "Accuracy_CEPr_Coauthor"   ,   "CEPr_Paper"      ,   "Accuracy_CEPr_Paper"  , "CEPr_Journal"    ,   "Accuracy_CEPr_Journal",
 "NB_Coauthor"  ,            "Accuracy_NB_Co"    ,    "NB_Paper"   ,        "Accuracy_NB_Paper" ,    "NB_Journal"   ,      "Accuracy_NB_Journal")
result[,c(2,4,6)]
```  
##Algorithm is
##(1) Sensitive to the volume of data  
The algorithm will yield better result if we have a relatively large citations from each person and every person sharing the same name has relatively equal amount of paper written.  
With only 2 citations per person, one for train and one for test, the accuracy is very low because we know almost nothing about this person.  
With person A having a lot of citations and person B having only a few, the features extracted (overlapping coauthors, term frequency of paper titles and journal titles) will be mostly about A, not B, since B will generate fewer coauthors and fewer paper title words and journal title words than A does.

##(2) Computationally expensive  
Assuming we have N papers with K different authors. In R, we were forced to reduce cluster tree level and the times we update at each level, which will decrease our accuracy greatly.  Also, it is required that at each level, we stop iteration only when lamda converges.
For sake of efficiency, we update lamda fixed times for each level. Also, we implemented a hard start, which means that we clustered the papers into small number of clusters using kmeans then we proceeded with agglomerative clustering to cluster the papers into K clusters. And we used parallel processing.     

## Suggestion for Further Study  
Use programming languages that are very suitable for loops and iterations.  
Use Parallel Processing whenever possible.  
Consider using a much larger database as source.  
If computational capacity allows, change the updating scheme of lamda to loop until convergence at each level as well as remove the hard start.



## Evaluate two algorithm  

```{r,echo=FALSE,warning=FALSE}
result = read.table("../data/test_result_1predictor.csv",as.is = T,sep = ",",header = T)
colnames(result) = c("TXT_ID", "CEPr_Coauthor"      ,      "Accuracy_CEPr_Coauthor"   ,   "CEPr_Paper"      ,   "Accuracy_CEPr_Paper"  , "CEPr_Journal"    ,   "Accuracy_CEPr_Journal",
 "NB_Coauthor"  ,            "Accuracy_NB_Co"    ,    "NB_Paper"   ,        "Accuracy_NB_Paper" ,    "NB_Journal"   ,      "Accuracy_NB_Journal")
result[,c(2,8,4,10,6,12)]
```
```{r,eval=FALSE}
#test set
# function Hier_clus_test() returns the accuracy and pairwise F1 score of the clustering partition chosen using best_lamda
test6 = Hier_clus_test(Test[[6]],Test[[6]][,-1],best_lamda[[6]],data = Test[[6]])
#accuracy = 60.3%
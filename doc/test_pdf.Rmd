---
title: "Project 4, Who is who"
author: "Ruxue Peng,rp2815"
date: "4/13/2017"
output: html_document
---  
In this project, we implement two of the suggested papers:  
(1) Han, Hui, et al. "Two supervised learning approaches for name disambiguation in author citations."  2004   
(2) Culotta, Aron, et al. "Author disambiguation using error-driven machine learning with a ranking loss function." 2007  

#Error Driven Paper 
The algorithm is "C_E_Pr"  
C: Clustering  
E:Error Driven  
Pr: Ranking perceptron lamda_t+1 = lamda_t + F(T)  

#### Basically, we defined a scoring system and performed agglomerative clustering on paper citations. We updated the scoring weight $lamda$ through ranking perceptron scheme. At last, we output the $best lamda$ for the scoring system so that we could find the best clustering partition in test set.  

## Step 0: Load the packages and fucntions

```{r, eval=FALSE,echo=FALSE}
setwd("~/GitHub/Spr2017-proj4-team2")
packages.used=c("plyr","dplyr","text2vec","qlcMatrix","kernlab", "parallel")
# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE,
                   repos='http://cran.us.r-project.org')
}

library(plyr)
library(text2vec)
library(qlcMatrix)
library(kernlab)
library(dplyr)
library(parallel)
library(ggplot2)
source("../lib/get_F1.R")
source("../lib/get_F1_accu.R")
source("../lib/get_labels_individual.R")
source("../lib/get_score_individual.R")
source("../lib/create_overlap.R")
source("../lib/Split_individual.R")
source("../lib/get_GoldStand.R")
source("../lib/hier_clus_train_individual.R")
source("../lib/hier_clus_for_test.R")
source("../lib/create_journal.R")
source("../lib/create_paper.R")
source("../lib/get_feature_individual.R")
source("../lib/Hier_clus_for_test.R")


```

## Step 1: Data Processing - Parse Txt Data
Following the experiment section in the paper, we used coauthors' names, paper titles as well as journal titles to design features for citations. A function named create_data() is written to automate this. Inside it, we used regular expressions.  

```{r,eval=FALSE,echo=FALSE}
#function for data pre-processing
create_data <- function(filename){
  #name <- deparse(substitute(filename))
  tmp <- read.csv(filename,
                  header = F,
                  sep = "\n")    
  rule = "<([[:alpha:]]|[[:punct:]]){1,4}>"
  tmp$V1 = gsub(rule,"",tmp$V1)
  rule1 = ">([[:alpha:]]){1,5}:"
  tmp$V1 = gsub(rule1,">",tmp$V1)
  Sys.setlocale('LC_ALL','C')
  L = strsplit(tmp$V1,split = ">")
  tmp$Coauthor = laply(L,function(t) t[1])
  tmp$Paper = laply(L,function(t) t[2])
  tmp$Journal = laply(L,function(t) t[3])
  
  # extract canonical author id befor "_"
  tmp$AuthorID <- as.numeric(sub("_.*","",tmp$Coauthor))
  # extract paper number under same author between "_" and first whitespace
  tmp$PaperNO <- as.numeric(sub(".*_(\\w*)\\s.*", "\\1", tmp$Coauthor))
  # delete "<" in AKumar$Coauthor, you may need to further process the coauthor
  # term depending on the method you are using
  tmp$Coauthor <- gsub("<","",sub("^.*?\\s","", tmp$Coauthor))
  # delete "<" in AKumar$Paper
  tmp$Paper <- gsub("<","",tmp$Paper)
  # add PaperID for furthur use, you may want to combine all the nameset files and 
  # then assign the unique ID for all the citations
  tmp$PaperID <- rownames(tmp)
  tmp = tmp[,-1]
  return(tmp)
}
```  
```{r,eval=FALSE}
## apply function
setwd("../data/nameset")
file_names <- list.files(pattern = "*.txt")
#file_names = file_names[-c(6,7,10)]
Data = list()
for(i in 1:length(file_names)){
  Data[[i]]= create_data(file_names[i])
}
names(Data) = file_names
rm(i)
```  

## Step 2: Feature Construction and PCA selection

We compute the number of overlapping coauthors between 2 papers, the "TF-IDF" of paper titles and also Journal titles as suggested in the paper.   
About "TF-IDF" (term frequency-inverse document frequency) 

TF-IDF is a numerical statistics that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in information retrieval, text mining, and user modeling. The TF-IDF value increases proportionally to the number of times a word appears in the document, but is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general.

$$
\begin{aligned}
\mbox{TF}(t) &=\frac{\mbox{Number of times term $t$ appears in a document}}{\mbox{Total number of terms in the document}}\\
\mbox{IDF}(t) &=\log{\frac{\mbox{Total number of documents}}{\mbox{Number of documents with term $t$ in it}}}\\
\mbox{TF-IDF}(t) &=\mbox{TF}(t)\times\mbox{IDF}(t)
\end{aligned}
$$   

### Feature Construction and Reduction   

To increase speed, we used PCA to reduce features and wrote feature construction function: get_feature(). For details of the function, please go to ../lib/get_feature.R 

```{r,eval=FALSE}
# f returns the PCAs with true labels on the 1st column
Fea = llply(Data,get_feature)
```  

### Split Train/Test  

We split each person's papers into half and half, put the first half in training set and the other half in the testing set.  

```{r,eval=FALSE}
##Split train and test
Train = llply(Fea,Split_Train)
Test = llply(Fea,Split_Test)
```  

## Step 3: Clustering  
Here we are trying to construct a scoring system that approximates the true score of a clustering method. If we have a good scoring system, we'll be able to pick the clustering that yields the highest score, and use its parition as prediction on the testing data, whose true assignments are unknown.  

In order to do that, we need to update the scoring weight (named $Lamda$) of each features.  

### Error Driven Training  
If our scoring system picks a clustering partition that is the best, we don't have to update lamda any more. If some other clustering method outperforms it, then it is said that we have an error and thus have to update our scoring weight. This is the basic idea of error driven online training. 
We will adopt hard start on the clustering, meaning that we'll use the true labels to create a base partitioning using kmeans.    
Then we use the base as a start for the hierarchical clustering.  

### Function for updating lamda   
To see the details of functions used within, please go to lib folder.  

```{r}
Hier_clus= function(Fea_wl,Fea_wol,Lamda,data = Train[[1]]){
  #get golden standard
  G = get_GoldStand(data)
  y = G$y
  a_c =G$a_c
  b_d = G$b_d 
  
  #hard start
  lamda = Lamda
  N = length(unique(y)) + 10
  init_cluster = kmeans(Fea_wl,centers = N,nstart =1)
  # initialization
  label_i= init_cluster$cluster
  
  for (v in 1: (N -length(unique(y)))){
    labels= get_labels(init_label= label_i,data = data)
    for(j in 1:5){
    #Parallel processing for greedy search
    cores <- detectCores()-1
    cl1 = makeCluster(cores,type = "FORK")
    score<- parApply(cl1,labels,2,get_score,lamda,Fea_wol)
    stopCluster(cl1)
    #score <- aaply(labels,2,get_score,lamda,Fea_wol)
      # extract the best score and N_hat and its F1 score
      ind_N_hat <- which.max(score)
      #ind_N_star <- which(F1==max(F1))
      label_i <-labels[,ind_N_hat]
      F1_N_hat = get_F1(label_i,A_C = a_c,B_D = b_d, data = data)
      
      # extract N_star
      for(i in 1:ncol(labels)){
        tmp = get_F1(init_label = labels[,i],A_C = a_c,B_D = b_d, data = data)
        if(tmp > F1_N_hat){
          ind_N_star = i
          break()
        }
        else{ind_N_star = ind_N_hat}
      }
      label_j <- labels[,ind_N_star]
      # update lamda
      Fea_wol_star <- data.frame(cbind(label_1=label_i,Fea_wol))
      Fea_wol_hat <- data.frame(cbind(label_1=label_j,Fea_wol))
      F_T_star <- colSums(aggregate(Fea_wol_star,by=list(Fea_wol_star$label_1),FUN=mean))[-c(1,2)]
      F_T_hat <- colSums(aggregate(Fea_wol_hat,by=list(Fea_wol_hat$label_1),FUN=mean))[-c(1,2)]
      incre <- sqrt(sum((F_T_hat-F_T_star)^2))
      lamda = lamda + 0.8*F_T_star - 0.8*F_T_hat
    }
  }
  return(lamda)
}

```  

### Train lamdas for all 14 txts  
Since each txt contains different paper title and journal title, they gave different features. Using the same lamda will not be reasonable and the length will not match. Therefore, we trained 14 different lamdas utilizing 14 txt files.  

```{r,eval=FALSE}
# initialize lamdas
best_lamda = llply(Train,function(t) rep(1/ncol(t[,-1]),ncol(t[,-1])))
Big_list = list()
for (i in 1:14) {
  Lam = matrix(rep(best_lamda6[[i]],nrow(Train[[i]])),byrow = T,nrow = nrow(Train[[i]]))
  Big_mat = cbind(Train[[i]],Lam)
  Big_list[[i]] =  Big_mat 
}
# store the 14 best_lamda vectors here:
best_lamda = llply(Big_list,function(t) Hier_clus(Fea_wl = t[,1:((ncol(t)+1)/2)],Fea_wol = t[,2:((ncol(t)+1)/2)],Lamda = t(t[1,((ncol(t)+3)/2):ncol(t)]),data = t[,1:((ncol(t)+1)/2)]))
```  

### Test on the other half of the data  

```{r,eval=FALSE}
#test set
# function Hier_clus_test() returns the accuracy and pairwise F1 score of the clustering partition chosen using best_lamda
test6 = Hier_clus_test(Test[[6]],Test[[6]][,-1],best_lamda[[6]],data = Test[[6]])
#accuracy = 60.3%
```  

## Step 4: Evaluation using pairwise F1 score  
#### Here we looks at algorithm's performance on different datasets, various feature types and combination of input parameters.  
We chose F1 score over accuracy, because the author informed us that they used pairwise F1 score as s_star and suggested that we should use that for evaluation. To better reproduce the result, we keep the criteria the same as before.  

### Different dataset  

Obviously, this does not do justice to the CEPr algorithm. If computation allows, better F1 can be obtained.
The variance of these F1s replects the algorithm's performance on different datasets, some have authors that wrote very few papers.  

#### After observing the txts, it is safe to say that the algorithm prefers dataset that have a lot of citations per person, and equal citation amounts between different persons.  

```{r,echo=FALSE}
result_cepr = read.table("../data/test_result_3predictor_cepr.csv",as.is = T,sep = ",",header = T)
colnames(result_cepr)[1] = "TXT_ID"
result_cepr = na.omit(result_cepr)
result_cepr[,1:2]
```  

### Different input parameters
We designed a way to evaluate the effect of Error Driven Online Training, excluding the effect of kmeans hard start.  
We train using the fifth txt, and two separate input parameters, for the first one, we update lamda twice at each level, for the other, 5 times.  

#### It is observed that the more we update lamda, the better F1 score we get.  

```{r}
F1_compare = c(Update_2 = 0.205, Update_5 = 0.232)
F1_compare
```  
### Different features  
The algorithm is not that sensitive to feature changes.  

```{r,echo=FALSE,warning=FALSE}
result = read.table("../data/test_result_1predictor.csv",as.is = T,sep = ",",header = T)
colnames(result) = c("TXT_ID", "CEPr_Coauthor"      ,      "Accuracy_CEPr_Coauthor"   ,   "CEPr_Paper"      ,   "Accuracy_CEPr_Paper"  , "CEPr_Journal"    ,   "Accuracy_CEPr_Journal",
 "NB_Coauthor"  ,            "Accuracy_NB_Co"    ,    "NB_Paper"   ,        "Accuracy_NB_Paper" ,    "NB_Journal"   ,      "Accuracy_NB_Journal")
result[,c(2,4,6)]
```  
##Algorithm is
##(1) Sensitive to the volume of data  

#### The algorithm will yield better result if we have a relatively large citations from each person and every person sharing the same name has relatively equal amount of paper written.  
With only 2 citations per person, one for train and one for test, the accuracy is very low because we know almost nothing about this person.  
With person A having a lot of citations and person B having only a few, the features extracted (overlapping coauthors, term frequency of paper titles and journal titles) will be mostly about A, not B, since B will generate fewer coauthors and fewer paper title words and journal title words than A does.

##(2) Computationally expensive  

#### Assuming we have N papers with K different authors. In R, we were forced to reduce cluster tree level and the times we update at each level, which will decrease our accuracy greatly.  Also, it is required that at each level, we stop iteration only when lamda converges.
For sake of efficiency, we update lamda fixed times for each level. Also, we implemented a hard start, which means that we clustered the papers into small number of clusters using kmeans then we proceeded with agglomerative clustering to cluster the papers into K clusters. And we used parallel processing.     

## Step 5: Suggestion for Further Study  

#### Use programming languages that are very suitable for loops and iterations.   
#### Use Parallel Processing whenever possible.  
#### Consider using a much larger database as source.  
#### If computational capacity allows, change the updating scheme of lamda to loop until convergence at each level as well as remove the hard start.



# Naive Bayes Paper  

### Load required libraries and the original dataset
```{r loading libraryies and Data, eval=FALSE,echo=FALSE}
library(dplyr)
library(plyr)
library(parallel)
library(ggplot2)
load("../data/Data.RData")
```

```{r load function, eval=FALSE,echo=FALSE}
source("../lib/Naive_A1_df.r")
source("../lib/Predict_model_A1.r")
source("../lib/get_train_test.r")
source("../lib/Log_A1_Value.r")
source("../lib/get_F1_accu.r")
source("../lib/dataclean.r")
source("../lib/dataprocess.r")
source("../lib/testclean.r")
source("../lib/testprocess.r")
source("../lib/get_F1.r")
source("../lib/get_GoldStand.r")
source("../lib/get_F1_accu.r")
source("../lib/dataclean3.r")
source("../lib/dataprocess3.r")

```

### Generate trainset and testset, random split the data into half and half in each group, each group here means each identity author ID.
```{r Generate trainset and test set, eval=FALSE}
set.seed(200)
train<- get_train_test(Data$JRobinson.txt)$train[,c(2,5)]
test<-get_train_test(Data$JRobinson.txt)$test[,c(2,5)]
```

# Naive Bayes Model

We assume that each author's citation data is generated by the naive Bayes model, and use his/her past citations as the training data to estimate the model parameters. Based on the parameter estimates, we use the Bayes rule to calculate the probability that each name entry $X_i(i\in [1,N])$, where $N$ is the total number of candidate name entries in the citation database) would have generated the input citation.\par

Given an input test citation $C$ with the omission of the query author, the target function is to find a name entry Xi in the citation database with the maximal posterior probability of producing the citation $C$, i.e.,

\begin{equation}
max_iP(x_i | C)
\end{equation}

Using the Bayes rule, the problem becomes finding

\begin{equation}
max_iP(x_i | C) = max_iP(C | X_i)P(X_i)/P(C)
\end{equation}

where $P(X_i)$ denotes the prior probability of Xi authoring papers, and is estimated from the training data as the proportion of the papers of $X_i$ among all the citations. 
\begin{equation}
P(X_i) = \frac{\text{# papers of $X_i$}}{\text{# all citations}}
\end{equation}
$P(C)$ denotes the probability of the citation C and is omitted since it does not depend on Xi. Then Function 2 becomes
\begin{equation}
P(C|X_i)=\prod_{j}P(A_j|X_i) = \prod_{j}\prod_{k}P(A_{jk}|X_i)
\end{equation}

where $A_j$ denotes the different type of attribute; that is, $A_1$ - the coauthor names; $A_2$ - the paper title; $A_3$ - the journal title. Each attribute is decomposed into independent elements represented by $A_{jk}$ $(k \in [0..K(j)])$. $K(j)$ is the total number of elements in attribute $A_j$.

To avoid underflow, we store log probabilities in our implementation, and the target function becomes:

\begin{equation}
max_iP(X_i|C)=max_i[\sum_{j}\sum_{k}\log(P(A_{jk}|X_i))+\log(P(X_i))]
\end{equation}

where $j\in[1,3]$ and $k\in[0,K(j)]$.

# Model parameters and Estimation

###Decomposition and estimation of the coauthor conditional probability $P(A_1|X_i)$
From the training citations, we calculate the following conditional probabilities.

$P(N|X_i)$ -the probability of $X_i$ writing a future paper alone conditioned on the event of $X_i$, estimated as the proportion of the papers that $X_i$ authors alone among all the papers of $X_i$. (N stands for "No coauthor", and "Co" below stands for "Has coauthor").
\begin{equation}
P(N|X_i) = \frac{\text{# of papers $X_i$ authors alone}}{\text{# of all the paper of $X_i$}}
\end{equation}

$P(Co|Xi)$ - the probability of $X_i$ writing a future paper with coauthors conditioned on the event of $X_i$.
\begin{equation}
P(Co|X_i) = 1-P(N|X_i)
\end{equation}

$P(Seen|Co,X_i)$ - the probability of $X_i$ writing a future paper with previously seen coauthors conditioned on the event that $X_i$ writes a future paper with coauthors. We regard the coauthors coauthoring a paper with $X_i$ at least twice in the training citations as the "seen coauthors"; the other coauthors coauthoring a paper with $X_i$ only once in the training citations is considered as the "unseen coauthors". Therefore, we estimate $P(Seen|Co,X_i)$ as the proportion of the number of times that $X_i$ coauthors with "seen coauthors" among the total number of times that $X_i$ coauthors with any coauthor. Note that if $X_i$ has n coauthors in a training citation $C$, we count that $X_i$ coauthors $n$ times in citation $C$.
\begin{equation}
P(Seen|Co,X_i) = \frac{\text{# of times $X_i$ coauthors with seen coauthors}}{\text{# of times $X_i$ coauthors with any coauthor}}
\end{equation}

$P(Unseen|Co,X_i)$ the probability of $X_i$ writing a future paper with "unseen coauthors" conditioned on the event that $X_i$ writes a paper with coauthors.
\begin{equation}
P(Unseen|Co,X_i) =1-P(Seen|Co,X_i)
\end{equation}

$P(A_{1k}|Seen,Co,X_i)$ - the probability of $X_i$ writing a future paper with a particular coauthor $A_{1k}$ conditioned on the event that $X_i$ writes a paper with previously seen coauthors. We estimate it as the proportion of the number of times that $X_i$ coauthors with $A_{1k}$ among the total number of times $X_i$ coauthors with any coauthor.
\begin{equation}
P(A1k|Seen,Co,X_i)=\frac{\text{# of times $X_i$ coauthors with $A_{1k}$}}{\text{# of times $X_i$ coauthors with seen coauthors}}
\end{equation}

$P(A_{1k}|Unseen,Co,X_i)$ - the probability of Xi writing a future paper with a particular coauthor $A_{1k}$ conditioned on the event that $X_i$ writes a paper with unseen coauthors. Considering all the names in the training citations as the population and assuming that $X_i$ has equal probability to coauthor with an unseen author, we estimate $P(A_{1k}|Unseen,Co,X_i)$ as 1 divided by the total number of author (or coauthor) names in the training citations minus the number of coauthors of {X_i}.
\begin{equation}
P(A_{1k}|Unseen,Co,X_i)=\frac{1}{\text{# of all authors}+\text{# of coauthors of $X_i$}}
\end{equation}

Based on above, creat a Data Frame $Author Data$ which includes coauthors names, author ID, and number of times the author coauthors with this specific coauthor. In addition, in this dataframe we include the $P(Co|X_i)$, $P(Seen | CO, X_i)$, $P(Unseen | Co, X_i)$, and the number of times $X_i$ coauthors with any seen coauthor. 
```{r Creat a A1 dictionary based on trainset, warning=FALSE,eval=FALSE}
a_train<-tapply(train$Coauthor,train$AuthorID,strsplit,split=";")
a_train<-subset(a_train,llply(a_train, length) != 1)
#class(a_train)

# Create a dataframe of author DJohnson

# number of indentity authors
n<-length(a_train)

Author_data<-data.frame()
for(i in 1:n){
  author<-df(a_train[[i]])
  author$author_id<-i
  Author_data<-rbind(Author_data,author)
}

# Remove author him/herself
Author_data<-Author_data[which(Author_data$Var1 != "YChen"),]
# Remove blanket
Author_data<-Author_data[which(Author_data$Var1 != ""),]

# Number of existing author ID, since maybe one author id just has one paper and doesn't have any coauthor
exist_author_id<-unique(Author_data$author_id)

m<-length(unique(Author_data$author_id))
```

Here we calculate the 5 probabilities based on the above dataframe. 
```{r,eval=FALSE}
# What we need to compute is prob_A1k_seencoxi
# Creat a dataframe seen_df which shows number of times xi coauthors with A1k
seen_df<-as.data.frame(matrix(NA,ncol = nrow(Author_data),nrow = m))
colnames(seen_df)<-Author_data$Var1

for (i in 1:m){
  for (j in 1: nrow(Author_data)){
    seen_df[i,j]<-ifelse(Author_data$author_id[j] == exist_author_id[i],
                         Author_data$numer_xi_seen_coau[j],0)}}

unique_coauthor<-unique(colnames(seen_df))
SEEN_DF<-matrix(NA, nrow = m,ncol = length(unique_coauthor))
colnames(SEEN_DF)<-unique_coauthor

for(i in 1:length(unique_coauthor)){
  
  logical_v<-colnames(seen_df) == unique_coauthor[i]
  
  if(sum(logical_v)>1){
    SEEN_DF[,i]<-rowSums(seen_df[,colnames(seen_df) == i])
  }else{
    SEEN_DF[,i]<-seen_df[,logical_v]
  }
}

# number of times xi coauthors with any seen coauthors
nxi_seen<-tapply(Author_data$numer_xi_seen_coau,Author_data$author_id,sum)

# p(seen | co,xi)
p_seen_coxi<-as.numeric(tapply(Author_data$prob_seen_cox1,Author_data$author_id,unique))
#p_seen_coxi[which(is.na(p_seen_coxi)|p_seen_coxi == Inf|p_seen_coxi == -Inf)]<-1

# p(co | xi)
p_co_xi<-as.numeric(tapply(Author_data$prob_cox1,Author_data$author_id,unique))

# p(unseen|co,xi)
p_unseen_coxi<-as.numeric(tapply(Author_data$prob_unseen_cox1,Author_data$author_id,unique))
#p_unseen_coxi[which(is.na(p_unseen_coxi)|p_unseen_coxi == Inf|p_unseen_coxi == -Inf)]<-0

# number of all authors and coauthors
all_a_c<-length(unique(Author_data$Var1))+n

# number of coauthors of xi
number_coa_xi<-rep(NA, m)
for(i in 1:m){
  number_coa_xi[i]<-nrow(Author_data[Author_data$author_id == exist_author_id[i],])
}

# p(A1k|unseen,co,xi)
p_A1k_unseencoxi<-1/(all_a_c - number_coa_xi)

# The first seen term
seen_term<-p_seen_coxi*p_co_xi

# The second term of object function p(A1k|unseen,co,xi)*p(unseen|co,xi)*p(co | xi) becomes
unseen_term<-p_A1k_unseencoxi*p_unseen_coxi*p_co_xi

# p(xi)
count_xi<-as.numeric(table(train$AuthorID))
p_xi<-count_xi/sum(count_xi)

# P(N|xi)
P_N_xi<-c()
for(i in 1:length(unique(train$AuthorID))){
  xi<-subset(train, AuthorID == i)
    l<-laply(laply(xi$Coauthor,strsplit,";"), length)
    P_N_xi[i]<-sum(l == 1)/length(l)}
```

Given a citation, which only include the coauthors names, we can predict the author ID based on Naive Bayes approch. First, we need to see if the given citation has any coauthor, if no, we say that the $P(A_1|X_i) = P(N|X_i)$, otherwise we calculate $P(A_{1k}|X_i) = P(A_{11}|X_i)...P(A_{1K}|X_{i})$, where

\begin{equation}
P(A_{1k}|X_i) = P(A_{1k},N|X_i)+P(A_{1k},Co|X_i) = 0+P(A_{1k},Co|X_i)=P(A_{1k}|Seen,Co,X_i)+P(A_{1k}|Unseen,Co,X_i) \\
=P(A_{1k}|Seen,Co,X_i)*P(Seen|Co,X_i)*P(Co|X_i)+P(A_{1k}|Unseen,Co,X_i)*P(Unseen|Co,X_i)*P(Co|X_i)
\end{equation}



and then calculate the object function $P(X_i|C)$, we find the one with the largest value which is our predict author ID.
```{r Model output label, warning=FALSE,eval=FALSE}
# Transfer above into a function
#Transfer the citation of into a data frame

predict_label<-rep(NA, nrow(test))
#predict_label<-c()
for(i in 1:nrow(test)){
  predict_label[i]<-citation_A1(test[i,1],"JRobinson")}
#predict_label

predict_label<-as.numeric(predict_label)
```

And then for the other two parts:
```{r, warning=FALSE,eval=FALSE}
testtemp<-testclean(test)
paper<-NULL
for(i in 1:nrow(test)){
  paper[i]<-strsplit(testtemp$Title[i],split = "\\s{1,3}" )
}
paper<-f3(paper)
pred_label<-judgement(paper)
```

###Decomposition and estimation of the coauthor conditional probability $P(A_2|X_i)$ and $P(A_3|X_i)$
Different from A_1, the other two features, the paper title and the journal title can be dealt with the same way but a little shorter. That is because you don't need to cae about such situation like "no title", we justneed to focus the appearance of the key words to find the probability.

\begin{equation}
P(A2k|X_i)=\frac{\text{# of times $X_i$ uses $A_{2k}$}}{\text{total # of times $A_{2k}$ was ever used}}
\end{equation}

\begin{equation}
P(A3k|X_i)=\frac{\text{# of times $X_i$ uses $A_{3k}$}}{\text{total # of times $A_{3k}$ was ever used}}
\end{equation}

What we're doing means that is we were given a test word, we can look up in our database to find the words used by different authors, and we supposed to believe that the words belongs to the author who used it most frequently.

Here we calculate the probabilities based on our trained dataset.

1.First to clean the data:
```{r,eval=FALSE}
data0<-Data[[5]]
data0<-dataprocess(data0)
train<-trainpro(data0)
test<-data0[-train$number,]
temp<-dataclean(train)
```

2.Compute the probabilities mentioned above:
```{r, eval=FALSE}
#total number of words
temp.num<-NULL
for(i in 1:nrow(temp)){
  temp.num<-paste(temp.num,temp$Title[i],sep = " ")
}
temp.num<-unlist(strsplit(temp.num,split = "\\s{1,10}"))
countnum<-length(unique(temp.num))

#All words ever appeared
temp.num<-unique(temp.num)[-1]

#Output of our dictionary
Fun13  =  function(word){adply(temp[,2],1,function(t) sum(word == unlist(strsplit(t,split = " "))))[,2]}
final = apply(as.matrix(temp.num),1,Fun13)
denom = matrix(rep(colSums(final),nrow(temp)),nrow = nrow(temp),byrow = T)
final = final/denom
colnames(final)<-temp.num

```
We can look up in the above constructed dictionary whenever we were given a test data.



### Get F1 Score and accuracy for A1,A2,A3
```{r,eval=FALSE}
#source("../lib/get_GoldStand.r")
#source("../lib/get_F1.r")

new_mydata<-data.frame(test[,2],test[,1])
colnames(new_mydata)<-c("AuthorID", "coauthor")
#View(new_mydata)
GoldStand<-get_GoldStand(new_mydata)
#F1<-rep(NA,12)
get_F1_accu(predict_label,new_mydata,GoldStand$a_c,GoldStand$b_d)
```

```{r,eval=FALSE}
new_mydata<-data.frame(test[,3],test[,2])
colnames(new_mydata)<-c("AuthorID","papertitle")
GoldStand<-get_GoldStand(new_mydata)
F1<-get_F1_accu(pred_label,new_mydata,GoldStand$a_c,GoldStand$b_d)
```

Here, we can see the result clearly in the barcharts:

Comparison between various features:
```{r}
mydata<-read.csv("../output/resultaccuracy.csv",header = T)

ggplot(mydata, aes(factor(txt_id), Accuracy, fill = Class)) + 
  geom_bar(stat="identity", position = "dodge") + 
  scale_fill_brewer(palette = "Set1")+
  xlab("text ID")+
  ggtitle("Accuracy rate for A1 A2 A3 in each txt")

```
Clearly, we see that the perfomance of journal title and paper title are quite similar, but both are a little bit different from the coauthor. This happens mainly because we have different feature extraction models,more restrictions requires a higher quality of the dataset. Those .txt files which have a lower accuracy rate always due to the poor data quality.
As for the journal and paper title part, since every author has their own habits of using words and reaerch areas,so if we detect the author od a test citation, we can easily find the right answer of the author.

We tried our hybrid models for some dataset and the result are showed below:
```{r}
mydata<-read.csv("../output/resultaccuracy2.csv",header = T)

ggplot(mydata, aes(factor(txt_id), F1score, fill = Class)) + 
  geom_bar(stat="identity", position = "dodge") + 
  scale_fill_brewer(palette = "Set1")+
  xlab("text ID")+
  ggtitle("F1 score for A1,A2,A3 and their combination in each txt")
```
The hybrid model gives us a higher accuracy mainly because of the more features are provided to judge for the result. The higher F1 score also certifies that the model has been improved but not overfit.

Comparison of the F1 and Accuracy in each feature:
```{r}
mydata<-read.csv("../output/co_p_j.csv",header = T)
mydata1<-mydata[1:28,]
mydata2<-mydata[29:56,]
mydata3<-mydata[57:84,]

ggplot(mydata1, aes(factor(txt_id), Value, fill = Class)) + 
  geom_bar(stat="identity", position = "dodge") + 
  scale_fill_brewer(palette = "Set2")+
  xlab("text ID")+
  ggtitle("Accuracy rate vs F1 score for coauthor")

ggplot(mydata2, aes(factor(txt_id), Value, fill = Class)) + 
  geom_bar(stat="identity", position = "dodge") + 
  scale_fill_brewer(palette = "Set2")+
  xlab("text ID")+
  ggtitle("Accuracy rate vs F1 score for paper title")

ggplot(mydata3, aes(factor(txt_id), Value, fill = Class)) + 
  geom_bar(stat="identity", position = "dodge") + 
  scale_fill_brewer(palette = "Set2")+
  xlab("text ID")+
  ggtitle("Accuracy rate vs F1 score for journal title")

```

We can see something interested in the above charts:
We see that for the JLee, all features have a high accuracy but they only got a F1 score, this may means that the model we constructed are overfit. After checking in the imput data we find that authors pretend to use different words when they stat to write a new  paper. This habits gives us a high dictionary but no outstanding probabilities. 
For MJones, we get both a high F1 and Accuracy. This is because the dataset is quite clean and every citation has enough information. Authors all tend to used old words instead of the new one which gives us more reliable probabilty dictionary.

So, we can say that Naive Bayes gives us better result when the dataset is better. This is a lttle bit tricky, but the algotithm does depend on the dataset quite a lot. When we get an ideal dataset we can get a perfect prediction result.
The algorithm do need a hugh dataset to save for the probabilty of the habit used words, coauthor names and something like that. So we need to collect enough information before we run for the algorithm. Besides, it can be changed hugely by the quality of the imput data which may not be a quite stable algorithm.

## Evaluate two algorithm  

```{r,echo=FALSE,warning=FALSE}
result = read.table("../data/test_result_1predictor.csv",as.is = T,sep = ",",header = T)
colnames(result) = c("TXT_ID", "CEPr_Coauthor"      ,      "Accuracy_CEPr_Coauthor"   ,   "CEPr_Paper"      ,   "Accuracy_CEPr_Paper"  , "CEPr_Journal"    ,   "Accuracy_CEPr_Journal",
 "NB_Coauthor"  ,            "Accuracy_NB_Co"    ,    "NB_Paper"   ,        "Accuracy_NB_Paper" ,    "NB_Journal"   ,      "Accuracy_NB_Journal")
result[,c(2,8,4,10,6,12)]
```
```{r,eval=FALSE}
#test set
# function Hier_clus_test() returns the accuracy and pairwise F1 score of the clustering partition chosen using best_lamda
test6 = Hier_clus_test(Test[[6]],Test[[6]][,-1],best_lamda[[6]],data = Test[[6]])
#accuracy = 60.3%
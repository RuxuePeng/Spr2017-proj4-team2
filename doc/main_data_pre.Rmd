---
title: "Project 4 - Example Main Script"
author: "Jing Wu, Tian Zheng"
date: "3/22/2017"
output: pdf_document
---

In this project, we implement one of the suggested papers, Culotta (2007). 

## Step 0: Load the packages, specify directories

```{r}
packages.used=c("plyr","text2vec","qlcMatrix","kernlab")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE,
                   repos='http://cran.us.r-project.org')
}

library(plyr)
library(text2vec)
library(qlcMatrix)
library(kernlab)
source("../lib/F1.R")
source("../lib/get_labels.R")
source("../lib/get_score.R")
source("../lib/create_overlap.R")
source("../lib/Split.R")



```
## Step 1: Load and process the data

For each record in the dataset, there are some information we want to extract and store them in a regular form: canonical author id, coauthors, paper title, publication venue title. You may need to find regular matched in the input string vectors by using regex in R. Here is a tutorial for regular expression in R, which might help you <https://rstudio-pubs-static.s3.amazonaws.com/74603_76cd14d5983f47408fdf0b323550b846.html>

```{r}
#function for data pre-processing
create_data <- function(filename){
  #name <- deparse(substitute(filename))
  tmp <- read.csv(filename,
                  header = F,
                  sep = "\n")    
  rule = "<([[:alpha:]]|[[:punct:]]){1,4}>"
  tmp$V1 = gsub(rule,"",tmp$V1)
  rule1 = ">([[:alpha:]]){1,5}:"
  tmp$V1 = gsub(rule1,">",tmp$V1)
  Sys.setlocale('LC_ALL','C')
  L = strsplit(tmp$V1,split = ">")
  tmp$Coauthor = laply(L,function(t) t[1])
  tmp$Paper = laply(L,function(t) t[2])
  tmp$Journal = laply(L,function(t) t[3])
  
  # extract canonical author id befor "_"
  tmp$AuthorID <- as.numeric(sub("_.*","",tmp$Coauthor))
  # extract paper number under same author between "_" and first whitespace
  tmp$PaperNO <- as.numeric(sub(".*_(\\w*)\\s.*", "\\1", tmp$Coauthor))
  # delete "<" in AKumar$Coauthor, you may need to further process the coauthor
  # term depending on the method you are using
  tmp$Coauthor <- gsub("<","",sub("^.*?\\s","", tmp$Coauthor))
  # delete "<" in AKumar$Paper
  tmp$Paper <- gsub("<","",tmp$Paper)
  # add PaperID for furthur use, you may want to combine all the nameset files and 
  # then assign the unique ID for all the citations
  tmp$PaperID <- rownames(tmp)
  tmp = tmp[,-1]
  return(tmp)
}
```

```{r}
## apply function
setwd("../data/nameset")
file_names <- list.files(pattern = "*.txt")
Data = list()
for(i in 1:length(file_names)){
  Data[[i]]= create_data(file_names[i])
}
names(Data) = file_names
```  

## Step 2: Feature Construction

Following the section 3.1 in the paper, we want to use paper titles to design features for citations. As the notation used in the paper, we want to find a $m$-dimensional citation vector $\alpha_i$ for each citation $i$, $i=1,...,n$. In this dataset, $n=$ `r nrow(AKumar)`. We study "TF-IDF" (term frequency-inverse document frequency) as suggested in the paper.

TF-IDF is a numerical statistics that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in information retrieval, text mining, and user modeling. The TF-IDF value increases proportionally to the number of times a word appears in the document, but is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general.

$$
\begin{aligned}
\mbox{TF}(t) &=\frac{\mbox{Number of times term $t$ appears in a document}}{\mbox{Total number of terms in the document}}\\
\mbox{IDF}(t) &=\log{\frac{\mbox{Total number of documents}}{\mbox{Number of documents with term $t$ in it}}}\\
\mbox{TF-IDF}(t) &=\mbox{TF}(t)\times\mbox{IDF}(t)
\end{aligned}
$$

To compute TF-IDF, we first need to construct a document-term matrix (DTM). In other words, the first step is to vectorize text by creating a map from words to a vector space. There are some good packages you could use for text mining (probably you have tried during first project, you don't need to follow my code if you are familiar with other package), e.g. *text2vec, tm, tidytext*. Here, we are going to use *text2vec* package. A good tutorial can be found here, <https://cran.r-project.org/web/packages/text2vec/vignettes/text-vectorization.html>.

Let???s first create a vocabulary-based DTM. Here we collect unique terms from all documents and mark each of them with a unique ID using the  `create_vocabulary()` function. We use an iterator to create the vocabulary.  

### compute overlapping coauthor
```{r}
overlap_data = create_overlap(Data[[1]][,1])
```  

### compute TF-IDF of paper title  

```{r}
it_train <- itoken(Data[[1]]$Paper,
             preprocessor = tolower, 
             tokenizer = word_tokenizer,
             ids = Data[[1]]$PaperID,
             # turn off progressbar because it won't look nice in rmd
             progressbar = FALSE)
vocab <- create_vocabulary(it_train, stopwords = c("a", "an", "the", "in", "on",
                                                   "at", "of", "above", "under"))
vectorizer <- vocab_vectorizer(vocab)
dtm_train <- create_dtm(it_train, vectorizer)
tfidf <- TfIdf$new()
dtm_train_tfidf <- fit_transform(dtm_train, tfidf)
paper_data = as.matrix(dtm_train_tfidf)
```  

### compute TF-IDF of Journal title  

```{r}
it_train <- itoken(Data[[1]]$Journal,
             preprocessor = tolower, 
             tokenizer = word_tokenizer,
             ids = Data[[1]]$PaperID,
             # turn off progressbar because it won't look nice in rmd
             progressbar = FALSE)
vocab <- create_vocabulary(it_train, stopwords = c("a", "an", "the", "in", "on",
                                                   "at", "of", "above", "under"))
vectorizer <- vocab_vectorizer(vocab)
dtm_train <- create_dtm(it_train, vectorizer)
tfidf <- TfIdf$new()
dtm_train_tfidf <- fit_transform(dtm_train, tfidf)
Journal_data = as.matrix(dtm_train_tfidf)
```  

### combine all the features  

```{r}
Fea_wol <- cbind(overlap_data,paper_data,Journal_data) 
#Fea_wl = cbind(label = unlist(Data[[1]]$AuthorID),Fea_wol)
```  

## Feature Reduction  
```{r}
# Use PCA to choose Top 100 most important features
Fea_small = prcomp(Fea_wol, center = T, scale. = T)
```  

## Split Train/Test  

```{r}
##Split train and test
Train = llply(Fea_wl,Split_Train)
Test = llply(Fea_wl,Split_Test)

```


## Step 3: Clustering

We will adopt hard start on the clustering, meaning that we'll use the true labels to create a base partitioning,  
which consists of 100 clusters. Then we use the base as a start for the hierarchical clustering on the matrix of the citation vectors.  

```{r}
start.time <- Sys.time()
init_cluster = kmeans(Fea_wl,centers = 50,nstart =5)
end.time <- Sys.time()
end.time-start.time
```

## calculate the match and mismatch of G
```{r}
y = Data[[1]]$AuthorID
df = expand.grid(1:nrow(dtm_train),1:nrow(dtm_train))
a_c = 0
for(i in 1:nrow(df)){
 q1 = df[i,1]
 q2 = df[i,2]
 a_c = a_c + (y[q1]==y[q2])
}
a_c = as.numeric(a_c)
b_d = choose(nrow(Data[[1]]),2)-a_c
```

## Compute predicitons matrix of all the partitioning in same level  

```{r}
labels = get_labels()
```

## Functions for "score" and "updating lamda" on each level
```{r}
# initialization of lamda vector 
s = Sys.time()
lamda = rep(1/ncol(Fea_wol),ncol(Fea_wol))
while(incre/ncol(Fea_wol) > 0.0003){
  score <- aaply(labels,2,get_score)
  # extract the best score and N_hat and its F1 score
  ind_N_hat <- which.max(score)
  #ind_N_star <- which(F1==max(F1))
  label_i <-labels[,ind_N_hat]
  F1_N_hat = get_F1(label_i)
  
  # extract N_star
  for(i in 1:ncol(labels)){
    tmp = F1(init_label = labels[,i])
    if(tmp>F1_N_hat){
      ind_N_star = i
      break()
    }
  }
  label_j <- labels[,ind_N_star]
  # update lamda
  Fea_wol_star <- data.frame(cbind(label_1=label_i,Fea_wol))
  Fea_wol_hat <- data.frame(cbind(label_1=label_j,Fea_wol))
  F_T_star <- colSums(aggregate(Fea_wol_star,by=list(Fea_wol_star$label_1),FUN=mean))[-c(1,2)]
  F_T_hat <- colSums(aggregate(Fea_wol_hat,by=list(Fea_wol_hat$label_1),FUN=mean))[-c(1,2)]
  incre <- sqrt(sum((F_T_hat-F_T_star)^2))
  lamda = lamda + F_T_star - F_T_hat
}
t = Sys.time()-s
#at last, we'll have the ultimate N_hat: label_i, lamda
```


We can also using hierarchical clustering method under the cosine distance. The intention of using a different clustering method is just to let you know how to compare performance between various methods. 
```{r}
start.time <- Sys.time()
docsdissim <- cosSparse(t(dtm_train_tfidf))
rownames(docsdissim) <- c(1:nrow(dtm_train_tfidf))
colnames(docsdissim) <- c(1:nrow(dtm_train_tfidf))
#compute pairwise cosine similarities using cosSparse function in package qlcMatrix
h <- hclust(as.dist(docsdissim), method = "ward.D")
result_hclust <- cutree(h,length(unique(Data[[1]]$AuthorID)))
end.time <- Sys.time()
time_hclust <- end.time - start.time
table(result_hclust)
```

## Step 4: Evaluation

To evaluate the performance of the method, it is required to calculate the degree of agreement between a set of system-output partitions and a set of true partitions. In general, the agreement between two partitioins is measured for a pair of entities within partitions. The basic unit for which pair-wise agreement is assessed is a pair of entities (authors in our case) which belongs to one of the four cells in the following table (Kang et at.(2009)):

\includegraphics[width=500pt]{matching_matrix.png}

Let $M$ be the set of machine-generated clusters, and $G$ the set of gold standard clusters. Then. in the table, for example, $a$ is the number of pairs of entities that are assigned to the same cluster in each of $M$ and $G$. Hence, $a$ and $d$ are interpreted as agreements, and $b$ and $c$ disagreements. When the table is considered as a confusion matrix for a two-class prediction problem, the standard "Precision", "Recall","F1", and "Accuracy" are defined as follows.

$$
\begin{aligned}
\mbox{Precision} &=\frac{a}{a+b}\\
\mbox{Recall}&=\frac{a}{a+c}\\
\mbox{F1} &=\frac{2\times\mbox{Precision}\times\mbox{Recall}}{\mbox{Precision}+\mbox{Recall}}\\
\mbox{Accuracy}&=\frac{a+d}{a+b+c+d}
\end{aligned}
$$

```{r}
source("../lib/evaluation_measures.R")
matching_matrix_hclust <- matching_matrix(AKumar$AuthorID,result_hclust)
performance_hclust <- performance_statistics(matching_matrix_hclust)
matching_matrix_sclust <- matching_matrix(AKumar$AuthorID,result_sclust)
performance_sclust <- performance_statistics(matching_matrix_sclust)
compare_df <- data.frame(method=c("sClust","hClust"),
                         precision=c(performance_sclust$precision, performance_hclust$precision),
                         recall=c(performance_sclust$recall, performance_hclust$recall),
                         f1=c(performance_sclust$f1, performance_hclust$f1),
                         accuracy=c(performance_sclust$accuracy, performance_hclust$accuracy),
                         time=c(time_sclust,time_hclust))
kable(compare_df,caption="Comparision of performance for two clustering methods",digits = 2)
```

